# Configuration for Political Polarity Pipeline
# All paths relative to project root

# Random seed for reproducibility
random_seed: 42

# Data paths
data:
  raw_dir: "./data/raw"
  processed_dir: "./data/processed"
  artifacts_dir: "./artifacts"
  
# Data preparation
preparation:
  test_size: 0.15
  val_size: 0.15
  min_text_length: 50  # characters
  max_text_length: 10000  # characters
  language: "en"
  stratify_by_source: true
  
# Label mapping
labels:
  left: 0
  center: 1
  right: 2
  class_names:
    - "Left/Democrat/Liberal"
    - "Center/Mixed/Unclear"
    - "Right/Republican/Conservative"

# Baseline model (TF-IDF + Logistic Regression)
baseline:
  max_features: 10000
  ngram_range: [1, 3]
  min_df: 5
  max_df: 0.8
  use_idf: true
  sublinear_tf: true
  classifier: "logistic"  # or "svm"
  C: 1.0
  class_weight: "balanced"
  calibration: "isotonic"  # or "sigmoid" or null
  top_features: 50  # per class for interpretability
  
# Transformer model
transformer:
  model_name: "distilbert-base-uncased"
  max_length: 256
  batch_size: 16
  learning_rate: 2.0e-5
  weight_decay: 0.01
  epochs: 3
  warmup_steps: 500
  gradient_accumulation_steps: 1
  fp16: true  # use mixed precision if GPU available
  dataloader_num_workers: 2
  save_steps: 500
  eval_steps: 500
  logging_steps: 100
  early_stopping_patience: 3
  
# Evaluation
evaluation:
  metrics:
    - "accuracy"
    - "macro_f1"
    - "precision_per_class"
    - "recall_per_class"
    - "f1_per_class"
  confusion_matrix: true
  calibration_curve: true
  calibration_bins: 10
  error_analysis_samples: 100
  
# Interpretability
interpretability:
  baseline_top_features: 30
  transformer_method: "attention"  # or "shap" or "leave_one_out"
  max_keywords: 20
  
# Subset analysis (k-anonymity)
privacy:
  min_group_size: 30  # k-anonymity threshold
  suppress_small_groups: true
  
# Inference
inference:
  batch_size: 32
  return_probabilities: true
  return_keywords: true
  confidence_threshold: 0.5  # for flagging uncertain predictions
  
# Phrase extraction
phrases:
  keybert_model: "all-MiniLM-L6-v2"
  top_n_keywords: 20
  keyphrase_ngram_range: [1, 3]
  diversity: 0.5  # for maximal marginal relevance
